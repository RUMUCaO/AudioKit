{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RUMUCaO/AudioKit/blob/main/stud_NLP_A2_24_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cihj8t1AM00f"
      },
      "source": [
        "# Lab 2: Sequence Modeling\n",
        "\n",
        "In this lab we will implement two methods of sequence modeling in NLP (and other applications). The methods differ radically but they have their merits.  \n",
        "The **Viterbi algorithm** for sequence modeling with a **Hidden Markov Model** recovers the most likely sequence underlying the input sequence, and while doing so it uses dynamic programming to find the optimal solution in an efficient way.  \n",
        "Recurrent Neural Networks were and are a simple and common deep learning approach for sequence modeling.\n",
        "They employ embeddings to represent words as vectors in a multidimensional space.  \n",
        "Let's dive into them...    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTkFjaarrc1F"
      },
      "source": [
        "# Rules\n",
        "\n",
        "You are greatly encouraged to add comments to your code describing what particular lines of code do (in general, a great habit to have in your coding life).\n",
        "Additionally, please follow these rules when submitting the notebook:\n",
        "\n",
        "* Put all code in the cell with the `# YOUR CODE HERE` comment.\n",
        "* For theoretical questions, put your solution in the `YOUR ANSWER HERE` or `ANSWER UNDER THIS LINE` cells (and keep the header if any).\n",
        "* Don't change or delete any initially provided cells, either text or code, unless explicitly instructed to do so.\n",
        "* Don't delete the comment lines `# TEST...` or edit their code cells. The test cells are for sanity checking. Passing them doesn't necessarily mean that your code is fine.\n",
        "* Don't change the names of provided functions and variables or arguments of the functions.\n",
        "* Don't clear the output of your code cells.\n",
        "* Don't output unnecessary info (e.g., printing variables for debugging purposes). This clutters the notebook and slows down the grading. You can have print() in the code, but comment them out before submitting the notebook.\n",
        "* Delete those cells that you inserted for your own debugging/testing purposes.\n",
        "* Don't forget to fill in the **contribution information**.\n",
        "* Don't forget to fill in the **work description section** per exercise.\n",
        "* Test your code and **make sure we can run your notebook** in the colab environment.\n",
        "* A single notebook file (without archiving) per group should be submitted via BB.\n",
        "\n",
        "<font color=\"red\">Following these rules helps us to grade the submissions relatively efficiently. If these rules are violated, a submission will be subject to penalty points.</font>  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWqs-t7Grc1L"
      },
      "source": [
        "# <font color=\"red\">Contributions</font>\n",
        "\n",
        "~~Delete this text and write instead of it yours:~~\n",
        "* ~~a list of group members' names (NOT student IDs)~~\n",
        "* ~~who contributed to which exercises and how. Note that it is important that each member has some contribution to each exercise, e.g., at least reviewing the solutions.~~\n",
        "\n",
        "YOUR ANSWER HERE [30-50 words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXUCfBV-9l7-"
      },
      "source": [
        "# General instructions\n",
        "\n",
        "Before diving into the exercises, keep in mind that the variables defined previously can be reused in the subsequent cells. So there is no need to redefine the same variable in multiple sections, e.g., it is sufficient to read the file in a variable once and later reuse the value of the variable, instead of re-reading the file.   \n",
        "\n",
        "If your code is too long and uses several code cells instead of a single code cell, rethink how to organize data in variables so that you can easily access the required info. Reading about [list comprehension](https://realpython.com/list-comprehension-python/#leverage-list-comprehensions) can be useful.\n",
        "\n",
        "Your code will often be evaluated based on its behaviour. So, during the grading some code cells are executed. If code runtime is too long than expected, this will hinder grading.\n",
        "\n",
        "<font color=\"red\">**The cases similar to the above-mentioned ones will be subject to penalty points.**</font>\n",
        "\n",
        "<font color=\"red\">**Pay attention to test units**</font> that are either provided as assert cases or as comments. Test units help you by giving you a hint about the correct answer. Note that **passing test units doesn't guarantee the full points** for an exercise because test units are incomplete, and the code might fail on other test units."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyN8CHaZiYZ4"
      },
      "source": [
        "# Part 1: Hidden Markov Model\n",
        "\n",
        "In this exercise, you will implement a **Hidden Markov Model (HMM)**. HMMs specify a **joint** probability over **observations** and **hidden states**.\n",
        "\n",
        "This is what we will do:\n",
        "\n",
        "1. **Estimate** a simple HMM model from training data (supervised learning)\n",
        "2. Find the *best* sequence of hidden states for a given sequence of observations (we define \"best\" in 2 different ways!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2bCocjNWhJe"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "N-hxVSot_94Q"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from collections import defaultdict, namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYtiw-uliYZ9"
      },
      "source": [
        "## Notation\n",
        "\n",
        "$ \\Sigma := \\{ o_1, \\dots, o_J \\} $ is our set of **observations**\n",
        "\n",
        "$\\Lambda := \\{s_1, \\dots, s_K \\}$ is our set of **state labels**\n",
        "\n",
        "$\\Sigma^*$ are **all possible sequences** of observations (including empty string $\\epsilon$)\n",
        "\n",
        "$\\Lambda^*$ all possible sequences of hidden states (including empty string $\\epsilon$)\n",
        "\n",
        "> Extra info: we can say that $\\Sigma^*$ is the [Kleene-closure](https://en.wikipedia.org/wiki/Kleene_star) of $\\Sigma$, and $\\Lambda^*$ the Kleene-closure of $\\Lambda$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU2wjdGviYZ_"
      },
      "source": [
        "## A simple example: The Baby HMM\n",
        "\n",
        "We start with a simple example, so that we can easily verify that our code is correct.\n",
        "\n",
        "Consider that we are modeling how a baby behaves. We observe the baby doing the following things: laughing (`laugh`), crying (`cry`), and sleeping (`sleep`). This is our set $\\Sigma$ of **observations**.\n",
        "\n",
        "We presume that, at any moment, the baby can be either `hungry`, `bored`, or `happy`. Since babies cannot talk, each of these states is hidden. This is our set $\\Lambda$ of **hidden states**.\n",
        "\n",
        "**Now the question is: if we have a series of observations, can we predict what hidden states the baby went through?**\n",
        "\n",
        "To tackle this problem, we assume that the baby behaves like a **1st order discrete Markov chain**: the baby's current state only depends on its previous hidden state. The baby can be described as an HMM. (Yay!)\n",
        "\n",
        "For example, assume we observed the baby doing the following:\n",
        "\n",
        "```\n",
        "sleep cry laugh cry\n",
        "cry cry laugh sleep\n",
        "```\n",
        "\n",
        "We will use these sequences as our **test set**. We can try to find out the states of the baby for those observations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmRqiS7WiYaB"
      },
      "source": [
        "Now, to train our model, we will need some examples of **observations** and **states**; this is our **training set**:\n",
        "\n",
        "```\n",
        "laugh/happy cry/bored cry/hungry sleep/happy\n",
        "cry/bored laugh/happy cry/happy sleep/bored\n",
        "cry/hungry cry/bored sleep/happy\n",
        "```\n",
        "\n",
        "So we have **pairs** observation/state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2RKaP-v_iYaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732a4404-64aa-411d-cb29-418480356558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test set (observations):\n",
            "['sleep', 'cry', 'laugh', 'cry']\n",
            "['cry', 'cry', 'laugh', 'sleep']\n",
            "\n",
            "training set (observation/state pairs):\n",
            "[laugh/happy, cry/bored, cry/hungry, sleep/happy]\n",
            "[cry/bored, laugh/happy, cry/happy, sleep/bored]\n",
            "[cry/hungry, cry/bored, sleep/happy]\n"
          ]
        }
      ],
      "source": [
        "# read in test data\n",
        "test_data = \"\"\"sleep cry laugh cry\n",
        "cry cry laugh sleep\"\"\"\n",
        "\n",
        "def test_reader(test_lines):\n",
        "    for line in test_lines.splitlines():\n",
        "        yield line.split()\n",
        "\n",
        "TEST_SET = list(test_reader(test_data))\n",
        "\n",
        "# read in train data\n",
        "train_data = \"\"\"laugh/happy cry/bored cry/hungry sleep/happy\n",
        "cry/bored laugh/happy cry/happy sleep/bored\n",
        "cry/hungry cry/bored sleep/happy\"\"\"\n",
        "\n",
        "# for convenience, we define a Observation-State pair class\n",
        "# fyi, more about namedtuple:\n",
        "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
        "Pair = namedtuple(\"Pair\", [\"obs\", \"state\"])\n",
        "Pair.__repr__ = lambda x: x.obs + \"/\" + x.state\n",
        "\n",
        "def train_reader(train_lines):\n",
        "    for line in train_data.splitlines():\n",
        "        # a pair is a string \"observation/state\" so we need to split on the \"/\"\n",
        "        yield [Pair(*pair.split(\"/\")) for pair in line.split()]\n",
        "\n",
        "TRAIN_SET = list(train_reader(train_data))\n",
        "\n",
        "# print the results\n",
        "print(\"test set (observations):\")\n",
        "for seq in TEST_SET:\n",
        "    print(seq)\n",
        "print(\"\\ntraining set (observation/state pairs):\")\n",
        "for seq in TRAIN_SET:\n",
        "    print(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ0zAoIgiYaF"
      },
      "source": [
        "## Vocabularies\n",
        "\n",
        "The `numpy` library is very practical when working with numeric vectors and matrices. Examples in this notebook heavily uses numpy arrays. It is recommended that you understand how simple arithmetic operation work on the arrays. You can also find very useful methods like `max` or `argmax` that will save several lines of code. You might find the following links useful: [NumPy Reference](https://numpy.org/doc/stable/reference/), [NumPy cheatsheets](https://blog.finxter.com/collection-10-best-numpy-cheat-sheets-every-python-coder-must-own/)\n",
        "\n",
        "It's going to be very useful if we can map states and observations to integers, so that we can identify them by a number. If we don't do this, then our implementation will be longer and much slower (This is relevant when we work with larger data, e.g. for POS tagging).\n",
        "\n",
        "Make sure you understand what is going on here: every time we look up an observation or state, the `defaultdict` will create a new key (index) if it has not seen that key (state or observation) before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AymV48GViYaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2336868-83df-4fb2-e1aa-aceccc2000a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Our vocabularies:\n",
            "{'happy': 0, 'bored': 1, 'hungry': 2}\n",
            "{'laugh': 0, 'cry': 1, 'sleep': 2}\n"
          ]
        }
      ],
      "source": [
        "# create mappings from state/obs to an ID\n",
        "STATE2INDX = defaultdict(lambda: len(STATE2INDX))\n",
        "OBS2INDX = defaultdict(lambda: len(OBS2INDX))\n",
        "\n",
        "for seq in TRAIN_SET:\n",
        "    for example in seq:\n",
        "        _ = STATE2INDX[example.state]\n",
        "        _ = OBS2INDX[example.obs]\n",
        "\n",
        "# convert defaultdict to normal dict to avoid their accidental modification\n",
        "STATE2INDX, OBS2INDX = dict(STATE2INDX), dict(OBS2INDX)\n",
        "\n",
        "print(\"\\nOur vocabularies:\")\n",
        "print(STATE2INDX)\n",
        "print(OBS2INDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUdGcxmdiYaI"
      },
      "source": [
        "The HMM for the first training set sequence looks like this:\n",
        "\n",
        "![hmm-baby-train-1.png](data:image/png;base64,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)\n",
        "\n",
        "Now, we will **estimate** the following probability distributions:\n",
        "\n",
        "- initial: $P( s_k \\,|\\, \\texttt{start})$\n",
        "- transition: $P( s_k \\,|\\, s_l )$\n",
        "- final: $P(\\texttt{stop} \\,|\\, s_k )$\n",
        "- emission: $P( o_l \\,|\\, s_k)$\n",
        "\n",
        "These distributions are all we need. Remember that:\n",
        "\n",
        "- the probability of transitioning to a state $s_k$ only depends on one previous state $s_l$ (1st order Markov assumption).\n",
        "- emitting an observation $o_l$ only depends on the state $s_k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5GorW8viYaK"
      },
      "source": [
        "## Finding the Maximum Likelihood Parameters\n",
        "\n",
        "Now we would like to know what those distributions look like from our data. This is called **estimation**. Given our training data, we count how many times each event occurs and normalize the counts to form proper probability distributions.\n",
        "\n",
        "Let's first do counts for the start probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A_NwY0mFiYaK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3cbce1-e6aa-4618-ba40-9438a3701cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "# we can get the number of states and observations from our dictionaries\n",
        "num_states = len(STATE2INDX)\n",
        "num_observations = len(OBS2INDX)\n",
        "\n",
        "# this creates a vector of length `num_states` filled with zeros\n",
        "counts_start = np.zeros(num_states)\n",
        "\n",
        "# now we count 1 every time a sequence starts with a certain state\n",
        "# we look up the index for the state that we want to count using the `STATE2INDX` dictionary\n",
        "for seq in TRAIN_SET:\n",
        "    counts_start[STATE2INDX[seq[0].state]] += 1.\n",
        "\n",
        "print(counts_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfaevoHniYaL"
      },
      "source": [
        "We see each state once at the beginning of a sequence in the training set, so that is why we have a count of 1 for each of them.\n",
        "\n",
        "We now **normalize** those counts, so that we obtain a probability distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A2mR5lI-iYaM",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a2b6e6-202e-48bf-b7be-bcffe2f1b08a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start --> [0.33333333 0.33333333 0.33333333]\n"
          ]
        }
      ],
      "source": [
        "# since p_start is a numpy object, we can call sum on it; easy!\n",
        "total = counts_start.sum()\n",
        "\n",
        "# normalize: divide each count by the total\n",
        "p_start = counts_start / total\n",
        "print(f\"start --> {p_start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpjkxRv2iYaM"
      },
      "source": [
        "We now turn to the **transition probabilities** and **stop probabilities**. We count, and then we normalize:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GpYLqNmDiYaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd498805-130c-4868-a5a0-106a5a0ba7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition counts:\n",
            "[[1. 2. 1.]\n",
            " [2. 0. 1.]\n",
            " [0. 1. 0.]]\n",
            "Final counts:\n",
            "[2. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "# we can transition from any state to any other state in principle,\n",
        "# so we create a matrix filled with zeros so that we can count any pair of states\n",
        "# in practice, some transitions might not occur in the training data\n",
        "counts_trans = np.zeros((num_states, num_states))\n",
        "\n",
        "# for the final/stop probabilities, we only need to store `num_states` values.\n",
        "# so we use a vector\n",
        "counts_stop = np.zeros(num_states)\n",
        "\n",
        "# now we count transitions, one sequence at a time\n",
        "for seq in TRAIN_SET:\n",
        "    for i in range(1, len(seq)):\n",
        "\n",
        "        # convert the states to indexes\n",
        "        prev_state = STATE2INDX[seq[i-1].state]\n",
        "        current_state = STATE2INDX[seq[i].state]\n",
        "\n",
        "        # count\n",
        "        counts_trans[current_state, prev_state] += 1.\n",
        "        # note that the order of states/indices in this matrix\n",
        "        # follows conditional probability order p(q_i|q_{i-1})\n",
        "        # not the transition matrix A_{i-1}_{i}\n",
        "\n",
        "# count final states\n",
        "for seq in TRAIN_SET:\n",
        "    state = STATE2INDX[seq[-1].state]\n",
        "    counts_stop[state] += 1.\n",
        "\n",
        "# print the counts\n",
        "print(\"Transition counts:\")\n",
        "print(counts_trans)\n",
        "\n",
        "print(\"Final counts:\")\n",
        "print(counts_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZrxBHQHiYaO"
      },
      "source": [
        "Now we can normalize again. We will need to collect the total counts per state.\n",
        "Take some time to understand that the totals consist of the transition counts AND the final counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "am85FysNiYaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d2f1ac-1cca-4025-af2a-27ce36c3b641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3. 3. 2.]\n"
          ]
        }
      ],
      "source": [
        "# Useful trick: np.sum(m, 0) sums matrix m along the first dimension (with index 0).\n",
        "# Note that after summing, the dimension disappears\n",
        "print(counts_trans.sum(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1XsW6QSIiYaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3d64fb-6422-404f-9570-e87c434afc04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total counts per state:\n",
            "[5. 4. 2.]\n",
            "\n",
            "Transition probabilities:\n",
            "[[0.2  0.5  0.5 ]\n",
            " [0.4  0.   0.5 ]\n",
            " [0.   0.25 0.  ]]\n",
            "\n",
            "Final probabilities:\n",
            "[0.4  0.25 0.  ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "total_per_state = counts_trans.sum(0) + counts_stop\n",
        "print(f\"Total counts per state:\\n{total_per_state}\\n\")\n",
        "\n",
        "# now we normalize\n",
        "# here '/' works one column at a time in the matrix\n",
        "p_trans = counts_trans / total_per_state\n",
        "print(f\"Transition probabilities:\\n{p_trans}\\n\")\n",
        "\n",
        "# here '/' divides the values in each corresponding index in the 2 vectors\n",
        "p_stop = counts_stop / total_per_state\n",
        "print(f\"Final probabilities:\\n{p_stop}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1cikCNxiYaQ"
      },
      "source": [
        "**So far so good!** Now let's take care of **emission probabilities**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R2UmIZEkiYaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e81595-317c-4b13-e37d-fe22e1fff0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emission counts:\n",
            "[[2. 0. 0.]\n",
            " [1. 3. 2.]\n",
            " [2. 1. 0.]]\n",
            "\n",
            "p_emiss:\n",
            "[[0.4  0.   0.  ]\n",
            " [0.2  0.75 1.  ]\n",
            " [0.4  0.25 0.  ]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# now we create a matrix to keep track of emission counts\n",
        "# in principle any states can emit any observation\n",
        "# so we need a matrix again:\n",
        "# 1st dimension is for observations, 2nd is for states\n",
        "# the dim. order follows conditional probabilities p(o|s)\n",
        "counts_emiss = np.zeros((num_observations, num_states))\n",
        "\n",
        "# count\n",
        "for seq in TRAIN_SET:\n",
        "    for obs, state in seq:\n",
        "        obs = OBS2INDX[obs]\n",
        "        state = STATE2INDX[state]\n",
        "        counts_emiss[obs][state] += 1.\n",
        "\n",
        "# normalize\n",
        "p_emiss = counts_emiss / counts_emiss.sum(0)\n",
        "\n",
        "print(f\"emission counts:\\n{counts_emiss}\\n\")\n",
        "print(f\"p_emiss:\\n{p_emiss}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbgSVREVq-9m"
      },
      "source": [
        "This is a good moment for a sanity check. First, take a look at the training set to see if these probabilities are correct, i.e. check if  for each state $s_k$: $$\\sum_l P(s_l \\,|\\, s_k) = 1.0$$ Note that this includes transitions to \"stop\" state, so you have to take those into account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsBjY3dyiYaT"
      },
      "source": [
        "## Decoding a sequence\n",
        "\n",
        "Ok, so we have estimated a model. Great. Now what? Well, now we can **decode**!\n",
        "\n",
        "Given an observation sequence $o_1, o_2, \\dots, o_N$, we want to find the sequence of hidden states $s^* = s^*_1, s^*_2, \\dots, s^*_N$ that **best** explains those observations.\n",
        "\n",
        "But what does \"best\" mean?\n",
        "\n",
        "1. If we are interested in the best **global** assignment of states to the sequence as a whole, we can use the **Viterbi** algorithm.\n",
        "2. If we care more about minimizing the **local** error of getting each $s_i$ right, we can use **posterior decoding** (also called *max marginal decoding*).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1W6UFPkb_LKV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQpYp-cCiYaU"
      },
      "source": [
        "### Decoding: The Viterbi Algorithm\n",
        "\n",
        "Viterbi gives us the best global hidden state sequence, i.e.:\n",
        "\n",
        "$$ \\begin{array}{lll}\n",
        "s^* &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N \\,|\\, o_1, o_2, \\dots, o_N ) \\\\\n",
        "    &=& \\arg\\max_{s = s_1, s_2, \\dots, s_N} P(s_1, s_2, \\dots, s_N, o_1, o_2, \\dots, o_N )\n",
        "\\end{array}$$\n",
        "\n",
        "To explain Viterbi we will make use of a **trellis**, a kind of graph that shows us the possible states for each time step.\n",
        "\n",
        "For our earlier example, the trellis looks like this:\n",
        "\n",
        "![hmm-baby-train-1-trellis.png](data:image/png;base64,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)\n",
        "\n",
        "Note, that we can now label the edges of this trellis with the following probabilities:\n",
        "\n",
        "- $P_{\\text start}(s_k \\,|\\, \\text{start})$ on the three edges from \"start\"\n",
        "- $P_{\\text stop}(\\text{stop} \\,|\\, s_k)$ on the three edges leading to \"stop\"\n",
        "- $P_{\\text trans}(s_k \\,|\\, s_{l})$ on each remaining edge from state $s_l$ to $s_k$\n",
        "- $P_{\\text emiss}(o_j \\,|\\, s_k)$ from each state $s_k$, to an observation $o_j$ made from that state (not shown here)\n",
        "\n",
        "Do you see that our trellis nicely shows the independence assumptions of the HMM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8qPpkVjiYaU"
      },
      "source": [
        "### A Naive approach to get the best sequence\n",
        "\n",
        "To see why the Viterbi algorithm is so useful, we can consider another way to calculate $s^*$:\n",
        "\n",
        "- Iterate over all possible state sequences (all ways to go from `start` to `stop`)\n",
        "    - Calculate the probability for that sequence\n",
        "    - Store the highest probability seen so far and its sequence\n",
        "- Return the sequence that had the maximum probability\n",
        "\n",
        "The problem with this approach is that there are a lot of possible sequences!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "CxfZN2UxiYaV"
      },
      "source": [
        "## The Viterbi algorithm\n",
        "\n",
        "*We use a slightly different notation here compared to the lecture.*\n",
        "\n",
        "So how do we find the path with the highest score? The idea is that we can use our trellis to represent an **exponential number of paths**. Since we are only interested in the highest-scoring path, for every state at every time step, we only need to keep track of the **highest** probability that can lead us to that state. We can disregard any other paths that lead to that state, since they will for sure not be part of the highest-scoring path.\n",
        "\n",
        "Viterbi uses **dynamic programming**. Here, that means that we will re-use probabilities that we have already computed, so we never have to compute the score for the same sub-problem multiple times.\n",
        "\n",
        "Let's start at the beginning.\n",
        "\n",
        "For the first time step, the **viterbi score** is the transition probability of reaching a state $s_k$ from \"start\", times the probability of emitting the first observation $o_1$ from that state:\n",
        "\n",
        "$$\\text{viterbi}(1, s_k) = P_{\\text start}( s_k \\,|\\, \\text{start}) \\times P_{\\text emiss}(o_1 \\,|\\, s_k)$$\n",
        "\n",
        "So, the Viterbi trellis represents the path with maximum probability in position $i$ when we are in state $y_i$ having observed $o_1, o_2, \\dots, o_i$, the observations up to and including that point.\n",
        "\n",
        "Now that we have the viterbi scores for all states of the first time step in our trellis, we can use the following **recursive formula** to get the scores for all other states, one time step at a time:\n",
        "\n",
        "$$\\text{viterbi}(i, s_k) = \\big( \\max_{s_l \\in \\Lambda} P_{\\text trans}(s_k | s_l) \\times \\text{viterbi}(i-1, s_l) \\big) \\times P_{\\text emiss}(o_i \\,|\\, s_k)$$\n",
        "\n",
        "Finally, for our final state \"stop\" we need to do something special, since there is no observation there:\n",
        "\n",
        "$$\\text{viterbi}(N+1, \\text{stop}) = \\max_{s_l \\in \\Lambda} P_{\\text stop}(\\text{stop} \\,|\\, s_l) \\times \\text{viterbi}(i-1, s_l)  $$\n",
        "\n",
        "This is all we need to know what probability the highest scoring path has! Do you see how the dynamic programming helps us to solve this task efficiently?\n",
        "\n",
        "## How did we get here?\n",
        "\n",
        "Once we reach the \"stop\" state we know the maximum probability, but we forgot how we got there! If you don't see this immediately, remember that, whenever we computed the viterbi score for a state, we took the maximum over all previous states' viterbi scores, times the transition from those states. But we didn't keep track of which state was actually selected in that \"max\" operation. So now that we are in \"stop\", we don't know how we got there.\n",
        "\n",
        "To solve this, we will use **backpointers**. Whenever we do a $\\max$, we store what state was selected by that max (i.e. the $\\arg\\max$):\n",
        "\n",
        "$$\\text{backtrack}(i, s_k) = \\arg\\max_{s_l \\in \\Lambda} P_{\\text trans}(s_k | s_l) \\times \\text{viterbi}(i-1, s_l)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFlBQv_74cbW"
      },
      "source": [
        "## From probabilities to log-probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "AGEiDOw0iYaW"
      },
      "source": [
        "Now you know enough to implement Viterbi! But before we start...  \n",
        "Because probabilities tend to get rather small when multiplying, causing numerical instabilities, we will use **log probabilities**. This means that, instead of multiplying, we can now **sum** probabilities, because:\n",
        "\n",
        "$$ \\log(uv) = \\log u + \\log v$$\n",
        "\n",
        "To get the probability of a  path trough our trellis from \"start\" to \"stop\", we can just **sum** the log-probabilities that we encounter. So, finding the best (\"Viterbi\") path means finding the path with the **highest score**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "flA1qns5iYaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be479ad-f048-43b8-800d-0393beed96c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before:\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "[[0.2  0.5  0.5 ]\n",
            " [0.4  0.   0.5 ]\n",
            " [0.   0.25 0.  ]]\n",
            "[0.4  0.25 0.  ]\n",
            "[[0.4  0.   0.  ]\n",
            " [0.2  0.75 1.  ]\n",
            " [0.4  0.25 0.  ]]\n",
            "After:\n",
            "[-1.09861229 -1.09861229 -1.09861229]\n",
            "[[-1.60943791 -0.69314718 -0.69314718]\n",
            " [-0.91629073        -inf -0.69314718]\n",
            " [       -inf -1.38629436        -inf]]\n",
            "[-0.91629073 -1.38629436        -inf]\n",
            "[[-0.91629073        -inf        -inf]\n",
            " [-1.60943791 -0.28768207  0.        ]\n",
            " [-0.91629073 -1.38629436        -inf]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-77e2f4085654>:5: RuntimeWarning: divide by zero encountered in log\n",
            "  return np.log(p_start), np.log(p_trans), np.log(p_stop), np.log(p_emiss)\n"
          ]
        }
      ],
      "source": [
        "def convert_to_log(p_start=None, p_trans=None, p_stop=None, p_emiss=None):\n",
        "    \"\"\"\n",
        "    Convert all probabilities to log-probabilities\n",
        "    \"\"\"\n",
        "    return np.log(p_start), np.log(p_trans), np.log(p_stop), np.log(p_emiss)\n",
        "\n",
        "print(f\"Before:\\n{p_start}\\n{p_trans}\\n{p_stop}\\n{p_emiss}\")\n",
        "\n",
        "# do the conversion\n",
        "lp_start, lp_trans, lp_stop, lp_emiss = \\\n",
        "    convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
        "\n",
        "print(f\"After:\\n{lp_start}\\n{lp_trans}\\n{lp_stop}\\n{lp_emiss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHdqKIKgiYaY"
      },
      "source": [
        "## Smoothing\n",
        "\n",
        "Some probabilities were 0.0, and the log function is not defined for zero, resulting in a **warning** or `-inf` (depending on your numpy version).\n",
        "\n",
        "To prevent this, we can add a small **smoothing** value to our **counts**, so that we never have a probability of zero.\n",
        "\n",
        "To make things easier, we define a `normalize_all` function below that does all the normalization again,\n",
        "but now adding a small value to all the counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Jr6sZmlpiYaY"
      },
      "outputs": [],
      "source": [
        "def normalize(x, smoothing=0.1, axis=0):\n",
        "    smoothed = x + smoothing\n",
        "    return smoothed / smoothed.sum(axis)\n",
        "\n",
        "def normalize_all(counts_start, counts_trans, counts_stop, counts_emiss, smoothing=0.1):\n",
        "    \"\"\"Normalize all counts to probabilities, optionally with smoothing.\"\"\"\n",
        "    p_start = normalize(counts_start, smoothing=smoothing)\n",
        "    p_emiss = normalize(counts_emiss, smoothing=smoothing)\n",
        "\n",
        "    counts_trans_smoothed = counts_trans + smoothing\n",
        "    counts_stop_smoothed = counts_stop + smoothing\n",
        "    total_trans_stop = counts_trans_smoothed.sum(0) + counts_stop_smoothed\n",
        "    p_trans = counts_trans_smoothed / total_trans_stop\n",
        "    p_stop = counts_stop_smoothed / total_trans_stop\n",
        "\n",
        "    return p_start, p_trans, p_stop, p_emiss\n",
        "\n",
        "\n",
        "# normalize with smoothing\n",
        "smoothing = 0.1\n",
        "p_start, p_trans, p_stop, p_emiss = normalize_all(\n",
        "    counts_start, counts_trans, counts_stop, counts_emiss, smoothing=smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VDTTwycPGncZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45d020c3-b34a-4996-b78b-87689f806938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed probabilities:\n",
            "[0.33333333 0.33333333 0.33333333]\n",
            "[[0.2037037  0.47727273 0.45833333]\n",
            " [0.38888889 0.02272727 0.45833333]\n",
            " [0.01851852 0.25       0.04166667]]\n",
            "[0.38888889 0.25       0.04166667]\n",
            "[[0.39622642 0.02325581 0.04347826]\n",
            " [0.20754717 0.72093023 0.91304348]\n",
            " [0.39622642 0.25581395 0.04347826]]\n",
            "Smoothed log-probabilities:\n",
            "[-1.09861229 -1.09861229 -1.09861229]\n",
            "[[-1.59108877 -0.7396672  -0.78015856]\n",
            " [-0.94446161 -3.78418963 -0.78015856]\n",
            " [-3.98898405 -1.38629436 -3.17805383]]\n",
            "[-0.94446161 -1.38629436 -3.17805383]\n",
            "[[-0.92576948 -3.76120012 -3.13549422]\n",
            " [-1.57239664 -0.32721291 -0.09097178]\n",
            " [-0.92576948 -1.36330484 -3.13549422]]\n"
          ]
        }
      ],
      "source": [
        "# convert to log-probabilities\n",
        "print(f\"Smoothed probabilities:\\n{p_start}\\n{p_trans}\\n{p_stop}\\n{p_emiss}\")\n",
        "\n",
        "lp_start, lp_trans, lp_stop, lp_emiss = convert_to_log(p_start=p_start, p_trans=p_trans, p_stop=p_stop, p_emiss=p_emiss)\n",
        "print(f\"Smoothed log-probabilities:\\n{lp_start}\\n{lp_trans}\\n{lp_stop}\\n{lp_emiss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11sJAepliYaa"
      },
      "source": [
        "## Ex1 [24pt] Implement the Viterbi algorithm\n",
        "\n",
        "You will now implement the Viterbi algorithm. Complete the function `viterbi(sequence, lp_start, lp_trans, lp_emiss, lp_stop)` below.\n",
        "\n",
        "**Input:** sequence ($o_1, ..., o_N$), $P_\\text{start}$, $P_\\text{trans}$, $P_\\text{emiss}$, $P_\\text{stop}$\n",
        "\n",
        "*Forward pass: compute the best path for every end state*\n",
        "\n",
        "- set $\\text{viterbi}(1, s_k)$ for each $s_k$\n",
        "- for $i=2$ to $N$, and for each $s_k$, set $\\text{viterbi}(i, s_k$) and $\\text{backtrack}(i, s_k)$\n",
        "- $\\text{max_prob} = \\max_{s_l} P_{\\text{stop}}(\\text{stop} \\,|\\, s_l) \\times viterbi(N, s_l)$\n",
        "\n",
        "*Backward pass: backtrack to get most likely path*\n",
        "- $\\hat{s}_N = \\arg\\max_{s_l} P_\\text{stop}(\\text{stop} \\,|\\, s_l) \\times viterbi(N, s_l)$\n",
        "- for $i = N-1$ to $1$: $\\hat{s}_i = \\text{backtrack}(i+1, \\hat{s}_{i+1})$\n",
        "\n",
        "**Output:** max_prob, Viterbi path $\\hat{s}_1, \\hat{s}_2, \\dots, \\hat{s}_N$\n",
        "\n",
        "<font color=\"red\">**You a free to re-use global variables, such as `OBS2INDX` or `STATE2INDX`, if needed, in the body of the function, instead of redefining them.**</font>\n",
        "\n",
        "<font color=\"red\">**It is mandatory to use numpy array operations where possible, instead of treating arrays as lists.**</font>  The latter will lead to an inefficient use of numpy array and a lengthy code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6AoL2DI2iYaa"
      },
      "outputs": [],
      "source": [
        "def viterbi(sequence, lp_start=None, lp_trans=None, lp_stop=None, lp_emiss=None):\n",
        "    \"\"\"\n",
        "    Compute the Viterbi sequence.\n",
        "    Note: you have to use log-probabilities!\n",
        "\n",
        "    Return:\n",
        "      - best_score (float) the log-probability of the best path\n",
        "      - best_path (int list) the best path as a list of state IDs\n",
        "      - trellis (float array) the trellis\n",
        "    \"\"\"\n",
        "\n",
        "    length = len(sequence)\n",
        "    num_states = len(lp_start)\n",
        "\n",
        "    # trellis to store Viterbi scores\n",
        "    # we store -inf as our initial scores since log(0)=-inf\n",
        "    trellis = np.full([length, num_states], -np.inf)\n",
        "\n",
        "    # backpointers to backtrack (to remember what prev. state caused the maximum score)\n",
        "    # we initialize with -1 values, to represent a non-existing index\n",
        "    backpointers = -np.ones([length, num_states], dtype=int)\n",
        "\n",
        "    # YOUR CODE HER\n",
        "    # Initialization step (t = 0)\n",
        "    first_obs_idx = sequence[0]\n",
        "    trellis[0] = lp_start + lp_emiss[:, first_obs_idx]\n",
        "\n",
        "    # Forward pass\n",
        "    for t in range(1, length):\n",
        "        obs_idx = sequence[t]\n",
        "        for curr_state in range(num_states):\n",
        "            scores = trellis[t - 1] + lp_trans[:, curr_state] + lp_emiss[curr_state, obs_idx]\n",
        "            trellis[t, curr_state] = np.max(scores)\n",
        "            backpointers[t, curr_state] = np.argmax(scores)\n",
        "\n",
        "    # Termination: add stop probability\n",
        "    final_scores = trellis[length - 1] + lp_stop\n",
        "    best_score = np.max(final_scores)\n",
        "    last_state = np.argmax(final_scores)\n",
        "\n",
        "    # Backtracking to recover the best path\n",
        "    best_path = [0] * length\n",
        "    best_path[length - 1] = last_state\n",
        "    for t in reversed(range(1, length)):\n",
        "        best_path[t - 1] = backpointers[t, best_path[t]]\n",
        "\n",
        "    return best_score, best_path, trellis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFKQ-RPEiYab"
      },
      "source": [
        "## Trying out Viterbi\n",
        "\n",
        "Once you have implemented the Viterbi algorithm, try it out on the following sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "TwfGXoDriYab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73dcecf-781d-4660-e168-b24f536ef45f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation= ['sleep', 'cry', 'laugh', 'cry']\n",
            "-9.624392165193209\n",
            "best state sequence= [np.int64(0), np.int64(1), np.int64(0), np.int64(1)]\n",
            "['happy', 'bored', 'happy', 'bored']\n"
          ]
        }
      ],
      "source": [
        "# TEST 1.1\n",
        "# Test out your Viterbi-algorithm here\n",
        "\n",
        "test_sequence = TEST_SET[0]\n",
        "test_sequence_idx = [OBS2INDX[o] for o in test_sequence]\n",
        "best_score, best_path, _ = viterbi(test_sequence_idx, lp_start=lp_start, lp_trans=lp_trans, lp_stop=lp_stop, lp_emiss=lp_emiss)\n",
        "\n",
        "print(\"observation=\", test_sequence)\n",
        "print(best_score)\n",
        "print(\"best state sequence=\", best_path)\n",
        "\n",
        "i2state = {v : k for k, v in STATE2INDX.items()}\n",
        "print([i2state[i] for i in best_path])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4moydQnHy6"
      },
      "source": [
        "## Work description for Ex1\n",
        "\n",
        "~~Describe your approach to solving the exercises, for example, what steps you took first, followed by subsequent actions, which parts you found most challenging or easy, any specific helpful assistance received from TAs, whether you used GenAI, to what extent, at what stage, which one, how helpful was it, etc.~~\n",
        "\n",
        "YOUR ANSWER HERE [100-200 words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH8nlrr0adyB"
      },
      "source": [
        "# Part 2: POS tagging with LSTM\n",
        "\n",
        "In the 2nd part, we will play with training a recurrent model for part of speech tagging. We start with implementing the evaluation and training procedures for LSTM-based tagger. Initially we will work on the toy data so that you understand the procedures. Later we use the Brown corpus with Universal POS tags. We will train a tagger on the data and analyse the training dynamics. Then we will use pre-trained word embeddings to initialize an LSTM. We will experiment with training taggers with and without pre-trained word vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3SOt9C3cF2D"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veXo1-nEgRcM"
      },
      "outputs": [],
      "source": [
        "import random, math\n",
        "import nltk\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# if any cell errors with \"A UTF-8 locale is required. Got ANSI_X3.4-1968\"\n",
        "# uncomment and run the next two lines\n",
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RJvGiFM7M46"
      },
      "outputs": [],
      "source": [
        "# torch-specific code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0x8eXNmcUXT"
      },
      "outputs": [],
      "source": [
        "print(f\"NLTK version: {nltk.__version__}\")\n",
        "print(f\"torch version: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xazspBAgWsY"
      },
      "outputs": [],
      "source": [
        "# download several data incorporated in NLTK\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UkhrxK27K4y"
      },
      "outputs": [],
      "source": [
        "# Some auxiliary functions that will be reused throughout the notebook\n",
        "\n",
        "def plot_numbers(num_listm, xlabel=\"Epochs\", ylabel=\"Loss\", label=None):\n",
        "    \"\"\" Visualizes a list of numbers as a line plot\n",
        "    \"\"\"\n",
        "    plt.plot(np.arange(len(num_listm)), num_listm, 'o-', markersize=2, label=label)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.xlabel(xlabel)\n",
        "    if label: plt.legend()\n",
        "\n",
        "# Let's plot the loss values and see how training on more epochs decreases the loss value\n",
        "def plot_results(res, parts=(\"train\", \"valid\"), metrics=(\"loss\", \"acc\"), title=None):\n",
        "    \"\"\" Shorthand for contrasting loss and accuracy numbers obtained on different datasets\n",
        "    \"\"\"\n",
        "    for m in metrics:\n",
        "        for p in parts:\n",
        "            plot_numbers(res[p][m], label=f\"{p} {m}\", ylabel=m)\n",
        "        if title: plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "def train_validation_split(sequence, train_ratio, valid_ratio, seed=0, seperate_tags=False):\n",
        "    \"\"\" returns two non-overlapping subpart of the input sequence.\n",
        "        The sizes of the parts satisfy the size ration constraint.\n",
        "        If seperate_tags is on, this means the input has format of [(w1,pos1), (w2,pos2)]\n",
        "        and it will be converted to [(w1,w2), (pos1,pos2)]\n",
        "    \"\"\"\n",
        "    assert train_ratio + valid_ratio <= 1\n",
        "    population = list(sequence)\n",
        "    n = len(population)\n",
        "    train_num, valid_num = math.floor(n * train_ratio), math.floor(n * valid_ratio)\n",
        "    random.seed(seed)\n",
        "    data = random.sample(population, train_num + valid_num)\n",
        "    if seperate_tags:\n",
        "        data = [ list(zip(*sent)) for sent in data ]\n",
        "    return data[:train_num], data[train_num:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imOSUJaxMUG1"
      },
      "source": [
        "## LSTM tagger\n",
        "\n",
        "Read the [pytorch tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) on sequence tagging with LSTM. It is very important that you practice how to read and understand the tutorials on web. Not all tutorials are written in a clear way, and understanding the overlooked parts requires practice and effort.\n",
        "Fortunately, the aforementioned tutorial is very well written. Though it uses many new operations related to tensors (=a multi-dimensional matrix containing elements of a single data type).\n",
        "\n",
        "Pytorch is well documented and almost any function, method, or class can be looked up [here](https://pytorch.org/docs/stable/index.html), in the top-left corner. For example, `torch.randn` is defined [here](https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn). Note that keywords in the code of the tutorial are hyperlinks with links to the corresponding documentation.\n",
        "\n",
        "The code below is copied from the tutorial page as we will reuse certain variables and functions. We only renamed the data variable name to indicate that it is toy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA_7zF4cNAkh"
      },
      "outputs": [],
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "toy_training_data = [\n",
        "    # Tags are: DET - determiner; NN - noun; V - verb\n",
        "    # For example, the word \"The\" is a determiner\n",
        "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "word_to_ix = {}\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in toy_training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "print(word_to_ix)\n",
        "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20NvL5eTcwbI"
      },
      "source": [
        "The code below is also copied from the tutorial but it incorporates two additions:\n",
        "\n",
        "1.   Fixing the torch's random generator seeds: see `nn_seed` and `em_seed`. These seeds control the initialization of `self.word_embeddings` from [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) and `self.lstm` from [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM). In the first case, the `vocab_size` number of `embedding_dim`-dimensional tensors are generated, while in the second case the LSTM network is initialized with random weights (note that `self.hidden2tag` is also randomized weights denoting `V` matrix, from J&M chapter 8, that is used to obtain output from the hidden layer for each time step). We make the LSTM initialization based on the seed to maintain replicability of the same results.\n",
        "\n",
        "2.   Making the class flexible to allow initialization from pre-trained word embeddings. `embedding` input can be a tuple that specifies the dimensions of the embedding layer or a ready tensor that records pre-trained word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuWNTJXRMN8j"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding, hidden_dim, tagset_size, nn_seed=0, em_seed=0):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        if isinstance(embedding, torch.Tensor):\n",
        "            self.word_embeddings = nn.Embedding.from_pretrained(embedding)\n",
        "            embedding_dim = self.word_embeddings.weight.size(1)\n",
        "        else:\n",
        "            torch.manual_seed(em_seed)\n",
        "            vocab_size, embedding_dim = embedding\n",
        "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        torch.manual_seed(nn_seed)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYGkk1OghOjh"
      },
      "source": [
        "## Ex2.1a [8pt] Evaluation\n",
        "\n",
        "The training (i.e. last) code in the [tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging) contains lines of code that we will reuse several times. That's why we are going to make specific functions out of them. The first function we will make takes a model and data (and other stuff) and returns predictions and metrics wrapped in a dictionary. read the function definition carefully for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0OyOdUIZen_"
      },
      "outputs": [],
      "source": [
        "def evaluate_tagger(model, data, w2i=None, t2i=None, get_tags=False, get_scores=False):\n",
        "    \"\"\" model - an NN model, in particular an LSTMTagger object\n",
        "        data - a sequence of pairs of a list of tokens and a list of POS tags\n",
        "        w2i - a dict mapping words to indices, when w2i is not specified this means that\n",
        "            tokens are already mapped to indices in data and no mapping is needed. This option\n",
        "            will be handy when evaluating a tagger several times on a data during the training.\n",
        "        t2i - a dict mapping POS tags to indices. When t2i is not specified this means that\n",
        "            pos tags are already mapped to indices in data, and also when returning tags\n",
        "            as an output, they won't be mapped to human readable tags, but instead returned\n",
        "            as indices.\n",
        "        get_tags - whether return predicted tags (i.e., most probable tag per token)\n",
        "        get_scores - whether to return actual tag scores for each token\n",
        "    return:\n",
        "        a dictionary with keys 'loss', 'acc', 'scores', 'tags'.\n",
        "        Note that the loss value is averaged across sentences: (loss_sen_1+...loss_sen_N)/N.\n",
        "        The keys are present in the dictionary for certain values according to the input.\n",
        "        For example, if get_tags=False, there shouldn't be 'tags' key in the dictionary.\n",
        "        As an example, see the reference output below.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frZtHkpsl43t"
      },
      "outputs": [],
      "source": [
        "# TEST Ex1.1\n",
        "# deterministically initialize LSTM tagger and predict the tags for the data\n",
        "toy_tagger = LSTMTagger((len(word_to_ix), EMBEDDING_DIM), HIDDEN_DIM, len(tag_to_ix))\n",
        "# evaluate tagger\n",
        "pred = evaluate_tagger(toy_tagger, toy_training_data, w2i=word_to_ix, t2i=tag_to_ix, get_tags=True, get_scores=True)\n",
        "print(pred)\n",
        "\n",
        "print(\"Tokens, gold tags, and predicetd most probable tag per token\")\n",
        "for (sent, tags1), tags2 in zip(toy_training_data, pred['tags']):\n",
        "    for seq in (sent, tags1, tags2):\n",
        "        print(''.join([ f\"{el:^8}\" for el in seq ]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk7AUpG9YqbB"
      },
      "source": [
        "Reference output:\n",
        "\n",
        "```\n",
        "{'loss': 1.0800790786743164, 'acc': 0.5555555555555556, 'tags': [['NN', 'NN', 'DET', 'NN', 'NN'], ['NN', 'DET', 'DET', 'NN']], 'scores': [tensor([[-1.0546, -1.0076, -1.2498],\n",
        "        [-1.0310, -1.0182, -1.2656],\n",
        "        [-0.9870, -1.0311, -1.3069],\n",
        "        [-1.1044, -0.9940, -1.2090],\n",
        "        [-1.0882, -0.9960, -1.2248]]), tensor([[-1.0523, -0.9922, -1.2727],\n",
        "        [-0.9906, -0.9994, -1.3451],\n",
        "        [-0.9478, -1.0155, -1.3856],\n",
        "        [-1.0980, -0.9716, -1.2449]])]}\n",
        "Tokens, gold tags, and predicetd most probable tag per token\n",
        "  The     dog     ate     the    apple\n",
        "  DET      NN      V      DET      NN\n",
        "   NN      NN     DET      NN      NN\n",
        "Everybody  read    that    book\n",
        "   NN      V      DET      NN\n",
        "   NN     DET     DET      NN\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jLZ7Rx0sftx"
      },
      "source": [
        "## Q2.1b [4pt] Explain predictions\n",
        "\n",
        "\n",
        "\n",
        "1.   *Given the above predictions from the tagger, is there any rationale for why the tagger predicts NN often?*\n",
        "2.   *What is the predicted probability distribution over the tags for \"that\" in the data*? In case your tagger's output is different from the reference output, make clear which output your answer is based on.\n",
        "\n",
        "\n",
        "\n",
        "<font color=\"red\">█████ ANSWER UNDER THIS LINE █████</font>\n",
        "\n",
        "~~(Don't remove the header)~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbR99UKAqIJz"
      },
      "source": [
        "## Ex2.2: [8pt] Training\n",
        "\n",
        "We need to write a function that trains a model on the data. This function later will be reused for training the tagger on different training sets. Again, reuse the code from the tutorial page to complete the body of the function. *Keep the learning rate same*.\n",
        "During training, the training data is isually shuffled, but here, for simplicity, *don't shuffle the data*, keep it as it is.\n",
        "Note that the function has to return the list of loss and accuracy values that is one item longer than the number of epochs.\n",
        "Don't forget to **reuse** `evaluate_tagger` function.\n",
        "Read the function definition for more details.\n",
        "\n",
        "**Hints**: the function presupposes training and evaluating a model on the same data several times. Make sure that you convert the data into tensors once and not for every epoch.\n",
        "Feel free to define sub-functions that will be reused for train and valid parts. This will make the code less redundant.\n",
        "The tqdm module gives a nice way to report the progress of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqbum92joqK"
      },
      "outputs": [],
      "source": [
        "def train_tagger(model, train, w2i, t2i,\n",
        "                 valid=None, epoch_num=10, verbose=False):\n",
        "    \"\"\" model - an NN model, in particular an LSTMTagger object\n",
        "        train - a sequence of pairs of a list of tokens and a list of POS tags\n",
        "        w2i - a dict mapping words to indices\n",
        "        t2i - a dict mapping POS tags to indices\n",
        "        valid - optional validation data structurally identical to train.\n",
        "            When the validation data is provided, the output also records results about it.\n",
        "        epoch_num - the number of times to train the model on the train data\n",
        "        verbose - a flag that makes the function print various useful info. For example,\n",
        "            it can be used to turn on/off the tqdm progress bar during the training.\n",
        "\n",
        "        return:\n",
        "            a dictionary with keys 'train' and 'valid', the existence of the latter depends\n",
        "            on whether the valid arg is provided. The values of the keys are dictionaries\n",
        "            with keys 'loss', 'acc', 'best_acc'. The first two have a list value of length epoch_num + 1.\n",
        "            This is because it includes the loss & acc of the initial model and the final too.\n",
        "            Note that the loss value should be averaged across sentences: (loss_sen_1+...loss_sen_N)/N.\n",
        "            'best_acc' keeps the max values from the 'acc' list.\n",
        "            See the reference output below as an example.\n",
        "    \"\"\"\n",
        "    ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSvoKxNPwdqc"
      },
      "outputs": [],
      "source": [
        "# TEST Ex1.2-1\n",
        "# defining a toy validation set\n",
        "toy_validation_data = [\n",
        "    (\"The dog read that book\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "    (\"Everybody ate the apple\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "# initialize the model\n",
        "trained_toy_tagger = LSTMTagger((len(word_to_ix), EMBEDDING_DIM), HIDDEN_DIM, len(tag_to_ix))\n",
        "# train and retrieve the loss values per epoch\n",
        "res = train_tagger(trained_toy_tagger, toy_training_data, word_to_ix, tag_to_ix, valid=toy_validation_data, epoch_num=200)\n",
        "print(f\"results: {res}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC57hG61gjWp"
      },
      "source": [
        "Reference output\n",
        "```\n",
        "results: {'train': {'loss': [1.0800790786743164, 1.0762487053871155, 1.072744369506836, 1.069520890712738, 1.0665384531021118, 1.0637630820274353, 1.0611646175384521, 1.0587173104286194, 1.0563986897468567, 1.0541892051696777, 1.052071988582611, 1.0500323176383972, 1.048057198524475, 1.0461354851722717, 1.0442574620246887, 1.0424144864082336, 1.0405990481376648, 1.0388041734695435, 1.0370243191719055, 1.0352540016174316, 1.0334884524345398, 1.0317234992980957, 1.029955267906189, 1.0281798839569092, 1.0263945162296295, 1.0245959758758545, 1.0227813720703125, 1.0209480822086334, 1.0190935730934143, 1.0172154307365417, 1.0153114199638367, 1.0133792757987976, 1.0114167928695679, 1.0094218254089355, 1.0073922872543335, 1.0053260326385498, 1.0032211244106293, 1.0010755360126495, 0.9988869428634644, 0.9966534376144409, 0.9943730235099792, 0.9920433461666107, 0.9896624088287354, 0.9872280955314636, 0.9847381114959717, 0.9821900725364685, 0.9795820713043213, 0.9769113957881927, 0.9741758704185486, 0.971373051404953, 0.968500405550003, 0.9655555188655853, 0.9625357389450073, 0.9594384133815765, 0.9562608897686005, 0.9530004262924194, 0.9496542811393738, 0.9462195336818695, 0.9426933825016022, 0.9390727579593658, 0.9353549182415009, 0.9315365552902222, 0.9276146292686462, 0.9235861897468567, 0.9194479286670685, 0.9151966571807861, 0.9108293950557709, 0.9063427448272705, 0.9017335474491119, 0.8969985246658325, 0.8921345770359039, 0.8871385455131531, 0.8820073008537292, 0.8767377138137817, 0.8713269233703613, 0.8657719492912292, 0.8600700199604034, 0.8542185723781586, 0.8482151329517365, 0.8420574069023132, 0.8357434272766113, 0.8292712569236755, 0.8226395547389984, 0.8158469200134277, 0.8088927268981934, 0.8017762303352356, 0.7944974899291992, 0.7870568335056305, 0.779455155134201, 0.7716937363147736, 0.7637743949890137, 0.7556994557380676, 0.747471958398819, 0.7390953898429871, 0.7305739521980286, 0.7219121754169464, 0.7131155133247375, 0.7041897773742676, 0.6951413750648499, 0.685977429151535, 0.6767054200172424, 0.6673334836959839, 0.657869964838028, 0.64832404255867, 0.6387049555778503, 0.6290224492549896, 0.6192863881587982, 0.6095071136951447, 0.5996948778629303, 0.5898602306842804, 0.5800137519836426, 0.5701659321784973, 0.5603272318840027, 0.5505081117153168, 0.5407186597585678, 0.5309689044952393, 0.5212685018777847, 0.5116268247365952, 0.5020530223846436, 0.49255549907684326, 0.4831426292657852, 0.47382208704948425, 0.4646011292934418, 0.455486536026001, 0.44648461043834686, 0.43760116398334503, 0.42884135246276855, 0.420210063457489, 0.4117113947868347, 0.4033491313457489, 0.3951266258955002, 0.38704653084278107, 0.379111185669899, 0.37132251262664795, 0.36368197202682495, 0.35619062185287476, 0.34884922206401825, 0.3416580855846405, 0.3346172273159027, 0.32772640883922577, 0.3209850490093231, 0.3143923729658127, 0.30794721841812134, 0.3016485273838043, 0.2954946607351303, 0.2894841134548187, 0.28361500799655914, 0.27788545936346054, 0.27229342609643936, 0.2668367102742195, 0.26151304692029953, 0.25632014125585556, 0.2512555569410324, 0.24631685763597488, 0.24150147289037704, 0.23680689930915833, 0.23223048448562622, 0.22776974737644196, 0.22342193126678467, 0.2191845253109932, 0.21505482494831085, 0.21103034913539886, 0.20710836350917816, 0.20328643918037415, 0.19956187903881073, 0.1959322690963745, 0.19239506870508194, 0.18894780427217484, 0.18558815866708755, 0.1823136806488037, 0.17912207543849945, 0.17601102590560913, 0.17297829687595367, 0.17002175748348236, 0.1671391949057579, 0.16432850062847137, 0.16158772259950638, 0.15891478210687637, 0.15630778670310974, 0.15376483649015427, 0.1512840986251831, 0.1488637775182724, 0.14650212973356247, 0.14419741183519363, 0.14194803684949875, 0.13975241407752037, 0.13760895282030106, 0.13551612943410873, 0.13347259163856506, 0.13147679716348648, 0.12952743843197823, 0.12762318551540375, 0.1257627233862877, 0.12394478917121887, 0.12216819450259209, 0.12043173983693123, 0.11873431876301765, 0.11707473546266556, 0.11545206606388092, 0.11386516317725182, 0.11231311783194542], 'acc': [0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'best_acc': 1.0}, 'valid': {'loss': [1.0846959352493286, 1.0808380246162415, 1.0773123502731323, 1.074073314666748, 1.0710812211036682, 1.0683015584945679, 1.0657045245170593, 1.0632638335227966, 1.0609570741653442, 1.0587645769119263, 1.0566694736480713, 1.0546568036079407, 1.0527135729789734, 1.050828456878662, 1.048991858959198, 1.0471948385238647, 1.0454300045967102, 1.0436905026435852, 1.0419704914093018, 1.04026460647583, 1.0385679006576538, 1.0368764400482178, 1.0351861119270325, 1.0334933400154114, 1.031795084476471, 1.0300881266593933, 1.0283698439598083, 1.0266374945640564, 1.0248886346817017, 1.0231209993362427, 1.0213320851325989, 1.0195199251174927, 1.0176822543144226, 1.015817105770111, 1.0139222741127014, 1.0119959115982056, 1.010036051273346, 1.008040428161621, 1.0060072541236877, 1.0039344429969788, 1.0018200278282166, 0.9996618628501892, 0.9974577724933624, 0.9952059388160706, 0.992904007434845, 0.9905498921871185, 0.9881412386894226, 0.9856758415699005, 0.9831514060497284, 0.9805654585361481, 0.9779156148433685, 0.975199431180954, 0.9724143147468567, 0.9695577323436737, 0.9666268825531006, 0.9636191427707672, 0.9605317711830139, 0.9573618173599243, 0.9541065096855164, 0.950762927532196, 0.9473278522491455, 0.9437984228134155, 0.940171480178833, 0.936443954706192, 0.9326125979423523, 0.9286741614341736, 0.9246255159378052, 0.9204634130001068, 0.9161844551563263, 0.9117855131626129, 0.9072633981704712, 0.9026147425174713, 0.8978364765644073, 0.8929254114627838, 0.8878785669803619, 0.8826929926872253, 0.8773658275604248, 0.8718942999839783, 0.8662759959697723, 0.8605084121227264, 0.854589432477951, 0.8485172688961029, 0.8422901332378387, 0.8359066545963287, 0.8293657600879669, 0.8226668834686279, 0.8158095479011536, 0.8087940514087677, 0.8016207814216614, 0.7942907512187958, 0.7868053317070007, 0.7791665494441986, 0.7713768780231476, 0.7634392380714417, 0.755357176065445, 0.7471347451210022, 0.7387764751911163, 0.7302875816822052, 0.7216735482215881, 0.7129406034946442, 0.7040953636169434, 0.6951448917388916, 0.6860966384410858, 0.6769585013389587, 0.667738676071167, 0.658445805311203, 0.6490887403488159, 0.6396763920783997, 0.6302180886268616, 0.6207231283187866, 0.6112009882926941, 0.6016610860824585, 0.5921128392219543, 0.5825656354427338, 0.5730288028717041, 0.5635112524032593, 0.5540220439434052, 0.5445697605609894, 0.5351628661155701, 0.5258093178272247, 0.5165170133113861, 0.5072932541370392, 0.49814508855342865, 0.4890791177749634, 0.48010173439979553, 0.4712185859680176, 0.46243521571159363, 0.45375658571720123, 0.44518721103668213, 0.43673138320446014, 0.42839284241199493, 0.4201749265193939, 0.41208066046237946, 0.4041126221418381, 0.3962731957435608, 0.38856421411037445, 0.38098737597465515, 0.3735438734292984, 0.3662347346544266, 0.35906076431274414, 0.3520224094390869, 0.3451198488473892, 0.3383530527353287, 0.3317219018936157, 0.3252258598804474, 0.3188643902540207, 0.3126366436481476, 0.30654168128967285, 0.30057843029499054, 0.2947455793619156, 0.28904177248477936, 0.28346555680036545, 0.27801529318094254, 0.27268923819065094, 0.26748569309711456, 0.26240262389183044, 0.2574382796883583, 0.25259044021368027, 0.24785716831684113, 0.24323634058237076, 0.23872575163841248, 0.23432327806949615, 0.23002667725086212, 0.2258337214589119, 0.2217421904206276, 0.21774984151124954, 0.21385441720485687, 0.21005363762378693, 0.20634539425373077, 0.20272734761238098, 0.1991974264383316, 0.1957533359527588, 0.1923929899930954, 0.1891142502427101, 0.18591507524251938, 0.18279334902763367, 0.17974704504013062, 0.1767742782831192, 0.17387297004461288, 0.17104127258062363, 0.16827736794948578, 0.16557937115430832, 0.16294549405574799, 0.1603739634156227, 0.157863087952137, 0.15541130304336548, 0.1530168540775776, 0.15067820623517036, 0.1483938843011856, 0.14616229385137558, 0.14398206025362015, 0.1418517380952835, 0.13976992666721344, 0.1377352774143219, 0.1357465460896492, 0.1338024064898491, 0.13190167769789696, 0.13004306703805923, 0.12822554260492325, 0.1264479085803032, 0.12470909580588341], 'acc': [0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.4444444444444444, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.5555555555555556, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.7777777777777778, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'best_acc': 1.0}}\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2InGBZla0aQ2"
      },
      "outputs": [],
      "source": [
        "# TEST Ex1.2-2\n",
        "# DON'T DELETE THE OUTPUT\n",
        "plot_results(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSi4NJbvf29m"
      },
      "source": [
        "## Brown corpus\n",
        "\n",
        "In this exercise we use the [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) for English annotated with the [Universal POS (UPOS) tags](https://www.nltk.org/book/ch05.html#a-universal-part-of-speech-tagset). We can access the data from an NLTK's corpus list. Note that the UPOS tagset contains much fewer tags than the PennTreebank POS tags. This makes UPOS tagging relatively easy.\n",
        "\n",
        "You are provided with a function `train_validation_split` which extracts two subsets from the given corpus. The size of the subsets can be specified in terms of the ratio to the entire corpus size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGHiCdWGICfz"
      },
      "outputs": [],
      "source": [
        "# defining the corpus view (of type ConcatenatedCorpusView), which can be iterated\n",
        "BROWN_UPOS = list(nltk.corpus.brown.tagged_sents(tagset='universal'))\n",
        "# Peeking inside the data\n",
        "print(BROWN_UPOS[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVVpPyTeDRhN"
      },
      "outputs": [],
      "source": [
        "# This is how to get training and validation parts of different sizes from the data\n",
        "# we use seperate_tags=True to format the extracted parts according to the format of the toy datasets,\n",
        "# i.e., putting tokens and their POS tags in separate lists\n",
        "sample_train_data, sample_valid_data = train_validation_split(BROWN_UPOS, 0.1, 0.1, seperate_tags=True)\n",
        "print(f\"Training data ({len(sample_train_data)}) and validation data ({len(sample_valid_data)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiTT8Lb_sKVC"
      },
      "source": [
        "## Ex2.3a [3pt] Train UPOS tagger\n",
        "\n",
        "Write a code that initializes a new LSTM tagger with the **embedding dimensionality 32** and **hidden layer dimensionality 16**.\n",
        "We are opting for the low-dimensional representations to keep the training time reasonable on CPUs.\n",
        "Note that you will also need to create the mappings of words and tags to indices based on the **training and validation data**.\n",
        "To be deterministic in creating the mappings, *sort tags and words* with `sorted` and then map sorted elements to indices. For example, this should map the token `!` and the tag `.` to 0s.\n",
        "Note that the word-to-index mapping should cover all words from train and valid data, i.e. there shouldn't be unseen words in valid data.\n",
        "\n",
        "Train the tagger on `sample_train_data` with **50 epochs** and evaluate on `sample_valid_data` using your `train_tagger` function. Save the training results in `upos_res` which will be plotted by the test cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6XFtGoT3B3X"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "### YOUR CODE HERE ###\n",
        "# save results in upos_res\n",
        "# should take under 20min on colab's cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wueZZK4YyQD"
      },
      "outputs": [],
      "source": [
        "# TEST Ex1.3\n",
        "# DON'T DELETE THE OUTPUT\n",
        "plot_results(upos_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yPQEqnauPmZ"
      },
      "source": [
        "## Q2.3b [2pt] Best model\n",
        "\n",
        "Based on the training dynamics of the accuracy scores on the training and validation parts, after which epoch (if any) is recommended to stop training and why? (give the exact epoch number in your answer if it is recommended to stop training)\n",
        "\n",
        "<font color=\"red\">█████ ANSWER UNDER THIS LINE █████</font>\n",
        "\n",
        "~~(Don't remove the header)~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTlphlLWWI-J"
      },
      "source": [
        "## GloVe embeddings\n",
        "\n",
        "In this section we are downloading the [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained static word embeddings. We will reuse it to initialize the embedding layer of an LSTM, as opposed to the random initialization we have been using so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hXYMKE7UQgf"
      },
      "outputs": [],
      "source": [
        "# downloading from the Dutch server (should be a bit faster than the original source)\n",
        "GLOVE_FILE = \"glove.6B.100d.txt\"\n",
        "!wget -nv https://naturallogic.pro/_files_/download/mNLP/{GLOVE_FILE}.bz2\n",
        "!bzip2 -dk glove.6B.100d.txt.bz2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xhYeEbHzgI_"
      },
      "outputs": [],
      "source": [
        "# Course-specific package\n",
        "! rm -rf assigntools\n",
        "! git clone https://github.com/kovvalsky/assigntools.git\n",
        "from assigntools.NLP.GloVe import GloVeEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-E1sIgIxSVZ"
      },
      "outputs": [],
      "source": [
        "# Loading vectors from the file\n",
        "glove = GloVeEmbeddings(GLOVE_FILE)\n",
        "print(glove.vectors.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXgzK-J0BOGy"
      },
      "source": [
        "GloVe comes with its own word-to-index mapping: `stoi` (strings to indices). You can access each word's vector as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o34_z8-9Ymq8"
      },
      "outputs": [],
      "source": [
        "word1 = \"human\" #@param {type:\"string\"}\n",
        "word2 = \"robot\" #@param {type:\"string\"}\n",
        "if word1 and word2:\n",
        "    tensor1 = glove.vectors[glove.stoi[word1]]\n",
        "    tensor2 = glove.vectors[glove.stoi[word2]]\n",
        "    sim = F.cosine_similarity(tensor1, tensor2, dim=0).item()\n",
        "    print(f\"similarity({word1}, {word2}) = {sim}\")\n",
        "elif word1:\n",
        "    print(f\"The index is {glove.stoi[word1]}\")\n",
        "    print(f\"The vector is {glove.vectors[glove.stoi[word1]]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vke3Zydfodn"
      },
      "source": [
        "## Ex2.4 [11pt] Random vs GloVe\n",
        "\n",
        "In this exercise we are comparing two LSTM-based UPOS taggers, one with randomly initialized embeddings while another reusing the GloVe embeddings.\n",
        "To bolster the comparison, we will restrict the data only to those tagged sentences that consist of the words covered by the GloVe vectors (Note that GloVe covers 400K words). This also helps to reduce the data and make the experiments feasible for the colab's CPU.\n",
        "\n",
        "Complete the designated code cell below so that it runs the following experiment:\n",
        "\n",
        "1.   For each train & valid data split initialize two LSTM-based taggers, with the random and GloVe embeddings.\n",
        "2.   Train each tagger with a predefined number of epochs. Obviously, you should reuse `train_tagger` function here.\n",
        "3.   After the training is done, record the max/best score across the epochs on the validation part (because the validation score better represents the model's capacity than the training data). Use `best_valid_acc` to keep track of the best scores for each type of embedding initialization.\n",
        "\n",
        "One important detail! This is an example of how to initialize an LSTM tagger with the GloVe word vectors that is expected to predict $N$ number of tags.\n",
        "Remember, this was the reason why we modified the LSTMTagger class from the PyTorch tutorial.\n",
        "\n",
        "```\n",
        "LSTMTagger(glove.vectors, HIDDEN_DIM, N)\n",
        "```\n",
        "\n",
        "To be deterministic in creating the mappings of tags and words to indices, sort tags and words with sorted and then map sorted elements to indices. For example, this should map the token `!` and the tag `.` to 0s. Note that the word-to-index mapping should cover all words from `GLOVE_BROWN`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3sfmPzOJ6xb"
      },
      "outputs": [],
      "source": [
        "EMB_DIM = glove.vectors.size(1)\n",
        "HID_DIM = 20\n",
        "valid_ratio = 0.2\n",
        "train_ratios = [0.01, 0.02, 0.04, 0.1, 0.2, 0.4, 0.8]\n",
        "epoch_num = 50\n",
        "# keep the best accuracy on the valid part here. Keys\n",
        "best_valid_acc = {'glove':[], 'random':[]}\n",
        "# For better contrast, we are keeping only those sentences in the Brown corpus for which we have GloVe vectors\n",
        "# and also keep only those sentences that have 3 and more tokens\n",
        "GLOVE_BROWN = [ sent for sent in BROWN_UPOS if len(sent) > 2 and all([ w in glove.stoi for w, _ in sent ]) ]\n",
        "print(f\"New data size = {len(GLOVE_BROWN)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4OEZkeS3GZn4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# It should take under 20mins\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8b6gH-MCb8oB"
      },
      "outputs": [],
      "source": [
        "# TEST 1.4\n",
        "# DON'T DELETE THE OUTPUT\n",
        "plot_numbers(best_valid_acc['glove'], label='glove', ylabel='accuracy', xlabel='data exp')\n",
        "plot_numbers(best_valid_acc['random'], label='random', ylabel='accuracy', xlabel='data exp')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aiv4DzkTNzJ"
      },
      "source": [
        "Based on the plots of the best accuracy scores on the validation part, how would you compare the use of randomized embeddings to the use of pre-trained embeddings?\n",
        "\n",
        "<font color=\"red\">█████ ANSWER UNDER THIS LINE █████</font>\n",
        "\n",
        "~~(Don't remove the header)~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHe9S1CTnaZl"
      },
      "source": [
        "## Work description for Ex2\n",
        "\n",
        "~~Describe your approach to solving the exercises, for example, what steps you took first, followed by subsequent actions, which parts you found most challenging or easy, any specific helpful assistance received from TAs, whether you used GenAI, to what extent, at what stage, which one, how helpful was it, etc.~~\n",
        "\n",
        "YOUR ANSWER HERE [100-200 words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qVlbZcDiYag"
      },
      "source": [
        "# Acknowledgments\n",
        "\n",
        "Most of the part 1 was developed in collaboration with Joost Bastings.  \n",
        "Later it was revised by Tejaswini Deoskar.  \n",
        "The recent updates by Lasha Abzianidze make the notebook more streamlined and foolproof from the grading and the large course perspectives.\n",
        "\n",
        "The initial version of part 2, by Denis Paperno, included the contrast of pretrained and random word embeddings on POS tagging (using the extended Penn Treebank POS tags). The assignment was built around the allennlp library.\n",
        "\n",
        "Since 2022-23 course, the assignment was substantially changed by Lasha Abzianidze. allennlp was replaced with pytorch and transformers library. Penn Treebank POS tagging was replaced with Universal POS tagging. Part 2 now teaches students how to build a neural network tagger instead of using out-of-the-box tagger. These changes also drastically decreased GPU/CPU processing time."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}